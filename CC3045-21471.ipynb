{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 2 - Inteligencia Artificial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sebastian Juárez - 21471\n",
    "Link al repo: https://github.com/SebasJuarez/CC3045/tree/Lab2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Regresión Lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿Por qué el modelo de Naive Bayes se le considera “naive”?\n",
    "\n",
    "La razon por la que se le considera \"naive\" es por que el algoritmo asume que rodas las caracteristicas son independientes entre sí, lo que casi nunca es cierto en muchos datos. Pero a pesar de esto, el algoritmo si es eficiente en la clasificacion de los textos y tambien en detectetar spam.\n",
    "\n",
    "2. Explique la formulación matemática que se busca optimizar en Support Vector Machine, además responda\n",
    "¿cómo funciona el truco del Kernel para este modelo? (Lo que se espera de esta pregunta es que puedan\n",
    "explicar en sus propias palabras la fórmula a la que llegamos que debemos optimizar de SVM en clase)\n",
    "\n",
    "La formulacion que se trata de optimizar es 1/2(||w||)^2 que puede llegar a maximizar el margen entre clases.\n",
    "El truco del Kernel, segun entiendo, transforma los datos de una manera en la que se evita pasar por un proceso de \"transformacion\" y asi evitar hacer un calculo que se haria de otra manera, o en otros terminos, crea un atajo.\n",
    "\n",
    "3. Investigue sobre Random Forest y responda\n",
    "a. ¿Qué tipo de ensemble learning es este modelo?\n",
    "\n",
    "En ese caso el Random Forest una un tipo llamado Bagging, que entrena varios arboles de decision con diferentes muestras de datos y al final combina las predicciones.\n",
    "\n",
    "b. ¿Cuál es la idea general detrás de Random Forest?\n",
    "\n",
    "La idea es que usando muchos arboles, separandolos por subconjuntos y caracteristicas, se promedie y que eso haga que se reduzca el sobreajuste.\n",
    "\n",
    "c. ¿Por qué se busca baja correlación entre los árboles de Random Forest?\n",
    "\n",
    "Si la correlacion es baja, esto significa que hay mejor generalizacion, lo que siginifica que todos cometan los mismos errores y eso reduce la varianza. Por esto es que tener baja la correlación es lo ideal en este metodo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes - Codigo desde cero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9739\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "# Función para limpiar texto\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Leer archivo y procesar dataset\n",
    "def load_dataset(file_path, split_ratio=0.8, seed=42):\n",
    "    random.seed(seed)\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            label, message = line.strip().split('\\t', 1)\n",
    "            data.append((label, clean_text(message)))\n",
    "    \n",
    "    random.shuffle(data)\n",
    "    split_index = int(len(data) * split_ratio)\n",
    "    return data[:split_index], data[split_index:]\n",
    "\n",
    "# Entrenar modelo Naive Bayes\n",
    "def train_naive_bayes(training_data):\n",
    "    word_counts = {'spam': defaultdict(int), 'ham': defaultdict(int)}\n",
    "    class_counts = {'spam': 0, 'ham': 0}\n",
    "    vocab = set()\n",
    "    \n",
    "    for label, message in training_data:\n",
    "        class_counts[label] += 1\n",
    "        words = message.split()\n",
    "        for word in words:\n",
    "            word_counts[label][word] += 1\n",
    "            vocab.add(word)\n",
    "    \n",
    "    return word_counts, class_counts, vocab\n",
    "\n",
    "# Predecir la clase de un mensaje\n",
    "def predict(message, word_counts, class_counts, vocab, alpha=1):\n",
    "    message_words = message.split()\n",
    "    total_messages = sum(class_counts.values())\n",
    "    log_prob_spam = math.log(class_counts['spam'] / total_messages)\n",
    "    log_prob_ham = math.log(class_counts['ham'] / total_messages)\n",
    "    \n",
    "    for word in message_words:\n",
    "        log_prob_spam += math.log((word_counts['spam'].get(word, 0) + alpha) / (sum(word_counts['spam'].values()) + alpha * len(vocab)))\n",
    "        log_prob_ham += math.log((word_counts['ham'].get(word, 0) + alpha) / (sum(word_counts['ham'].values()) + alpha * len(vocab)))\n",
    "    \n",
    "    return 'spam' if log_prob_spam > log_prob_ham else 'ham'\n",
    "\n",
    "# Evaluar el modelo\n",
    "def evaluate(test_data, word_counts, class_counts, vocab):\n",
    "    correct = 0\n",
    "    total = len(test_data)\n",
    "    for label, message in test_data:\n",
    "        prediction = predict(message, word_counts, class_counts, vocab)\n",
    "        if prediction == label:\n",
    "            correct += 1\n",
    "    return correct / total\n",
    "\n",
    "train_data, test_data = load_dataset('data/entrenamiento.txt')\n",
    "word_counts, class_counts, vocab = train_naive_bayes(train_data)\n",
    "accuracy = evaluate(test_data, word_counts, class_counts, vocab)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Clasificación de nuevos mensajes\n",
    "while True:\n",
    "    msg = input(\"Ingrese un mensaje (o 'salir' para salir): \")\n",
    "    if msg.lower() == 'salir':\n",
    "        break\n",
    "    msg_clean = clean_text(msg)\n",
    "    prediction = predict(msg_clean, word_counts, class_counts, vocab)\n",
    "    print(f'Clasificación: {prediction}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes - Codigo con librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (sklearn): 0.9856\n"
     ]
    }
   ],
   "source": [
    "# Función para limpiar texto\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Leer archivo y procesar dataset\n",
    "def load_dataset(file_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            label, message = line.strip().split('\\t', 1)\n",
    "            data.append(clean_text(message))\n",
    "            labels.append(label)\n",
    "    return data, labels\n",
    "\n",
    "data, labels = load_dataset('data/entrenamiento.txt')\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "X_test_counts = vectorizer.transform(X_test)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_counts, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_counts)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy (sklearn): {accuracy:.4f}')\n",
    "\n",
    "# Clasificación de nuevos mensajes\n",
    "while True:\n",
    "    msg = input(\"Ingrese un mensaje (o 'exit' para salir): \")\n",
    "    if msg.lower() == 'exit':\n",
    "        break\n",
    "    msg_clean = clean_text(msg)\n",
    "    msg_vector = vectorizer.transform([msg_clean])\n",
    "    prediction = model.predict(msg_vector)[0]\n",
    "    print(f'Clasificación: {prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM - Codigo desde cero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy SVM (sin librerías): 1.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Cargar dataset\n",
    "def load_lol_dataset(file_path, split_ratio=0.8, seed=42):\n",
    "    random.seed(seed)\n",
    "    data = np.genfromtxt(file_path, delimiter=',', skip_header=1)\n",
    "    np.random.shuffle(data)\n",
    "    \n",
    "    split_index = int(len(data) * split_ratio)\n",
    "    return data[:split_index, :-1], data[:split_index, -1], data[split_index:, :-1], data[split_index:, -1]\n",
    "\n",
    "def linear_kernel(x1, x2):\n",
    "    return np.dot(x1, x2)\n",
    "\n",
    "# Implementación de SVM desde cero\n",
    "def train_svm(X, y, lr=0.001, lambda_param=0.01, epochs=1000):\n",
    "    n_samples, n_features = X.shape\n",
    "    weights = np.zeros(n_features)\n",
    "    bias = 0\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        for i, x_i in enumerate(X):\n",
    "            condition = y[i] * (np.dot(x_i, weights) - bias) >= 1\n",
    "            if condition:\n",
    "                weights -= lr * (2 * lambda_param * weights)\n",
    "            else:\n",
    "                weights -= lr * (2 * lambda_param * weights - np.dot(x_i, y[i]))\n",
    "                bias -= lr * y[i]\n",
    "    \n",
    "    return weights, bias\n",
    "\n",
    "def predict_svm(X, weights, bias):\n",
    "    approx = np.dot(X, weights) - bias\n",
    "    return np.sign(approx)\n",
    "\n",
    "# Evaluación del modelo\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_lol_dataset('data/high_diamond_ranked_10min.csv')\n",
    "y_train = np.where(y_train == 0, -1, 1)  # Convertir 0s a -1 para SVM\n",
    "y_test = np.where(y_test == 0, -1, 1)\n",
    "\n",
    "weights, bias = train_svm(X_train, y_train)\n",
    "y_pred = predict_svm(X_test, weights, bias)\n",
    "\n",
    "print(f'Accuracy SVM (sin librerías): {accuracy(y_test, y_pred):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM - Codigo con librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Entrenar modelo SVM con sklearn\u001b[39;00m\n\u001b[0;32m     17\u001b[0m svm_model \u001b[38;5;241m=\u001b[39m SVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m \u001b[43msvm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m svm_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     21\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1349\u001b[0m     )\n\u001b[0;32m   1350\u001b[0m ):\n\u001b[1;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:199\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    191\u001b[0m         X,\n\u001b[0;32m    192\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    196\u001b[0m         accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    197\u001b[0m     )\n\u001b[1;32m--> 199\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[0;32m    202\u001b[0m     [] \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[0;32m    203\u001b[0m )\n\u001b[0;32m    204\u001b[0m solver_type \u001b[38;5;241m=\u001b[39m LIBSVM_IMPL\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:739\u001b[0m, in \u001b[0;36mBaseSVC._validate_targets\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_targets\u001b[39m(\u001b[38;5;28mself\u001b[39m, y):\n\u001b[0;32m    738\u001b[0m     y_ \u001b[38;5;241m=\u001b[39m column_or_1d(y, warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 739\u001b[0m     \u001b[43mcheck_classification_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    740\u001b[0m     \u001b[38;5;28mcls\u001b[39m, y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_, return_inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    741\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight_ \u001b[38;5;241m=\u001b[39m compute_class_weight(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight, classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, y\u001b[38;5;241m=\u001b[39my_)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\multiclass.py:221\u001b[0m, in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    213\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m ]:\n\u001b[1;32m--> 221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown label type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Maybe you are trying to fit a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier, which expects discrete classes on a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression target with continuous values.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    225\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Cargar dataset de League of Legends\n",
    "def load_lol_dataset(file_path):\n",
    "    data = np.genfromtxt(file_path, delimiter=',', skip_header=1)\n",
    "    return data[:, :-1], data[:, -1]\n",
    "\n",
    "X, y = load_lol_dataset('data/high_diamond_ranked_10min.csv')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar modelo SVM con sklearn\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy SVM (con sklearn): {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se compararon los dos algoritmos. En el primer algoritmo pudimos ver que los dos tuvieron buenos resultados pero el que se hizo usando las librerias tuvo mejores resulatados, como se puede esperar de la libreria. En el segundo algoritmo, podemos ver que el realizado desde cero tuvo buenos resultados que alcanzararon los valores esperados, pero creo que ocurrieron problemas ya que en el primer algoritmo dio una respuesta de 1 en accuracy pero en el segundo no pudo procesarse seguramente por problemas del dataset que se debe arreglar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
